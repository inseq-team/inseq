import logging
import re
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

from rich import print as rprint
from rich.prompt import Confirm, Prompt

from .. import (
    AttributionModel,
    list_aggregation_functions,
    list_aggregators,
    list_step_functions,
)
from ..models import HuggingfaceModel
from ..utils import cli_arg
from ..utils.alignment_utils import compute_word_aligns
from .attribute import AttributeBaseArgs
from .base import BaseCLICommand

logger = logging.getLogger(__name__)


@dataclass
class AttributeContextArgs(AttributeBaseArgs):
    input_current_text: str = cli_arg(
        default="",
        help=(
            "The input text used for generation. If the model is a decoder-only model, the input text is a "
            "prefix used for language modeling. If the model is an encoder-decoder model, the input text is the "
            "source text provided as input to the encoder."
        ),
    )
    context_sensitivity_metric: str = cli_arg(
        default="kl_divergence",
        help="The metric used to detect context-sensitive tokens in generated texts.",
        choices=list_step_functions(),
    )
    input_context_text: Optional[str] = cli_arg(
        default=None,
        help="An input context for which context sensitivity should be detected.",
    )
    input_template: str = cli_arg(
        default="{context} {current}",
        help=(
            "The template used to format model inputs. The template must contain at least the"
            " ``{current}`` placeholder, which will be replaced by ``input_current_text``. If ``{context}`` is"
            " also specified, source-side context will be used. Useful for models requiring special tokens or"
            " formatting in the input text (e.g. <brk> tags to separate context and current inputs)."
        ),
    )
    output_context_text: Optional[str] = cli_arg(
        default=None,
        help=(
            "An output contexts for which context sensitivity should be detected, in case the model"
            " accepts target-side outputs (e.g. context-aware MT)."
        ),
    )
    output_current_text: Optional[str] = cli_arg(
        default=None,
        help=(
            "The output text generated in the contextual setting, for which context dependence will be identified"
            "and attributed. If specified, this output is force-decoded. Otherwise, the output is generated by the"
            "selected model using the default generation strategy."
        ),
    )
    output_template: str = cli_arg(
        default="{current}",
        help=(
            "The template used to generate the input text for the model. The template must contain at least the"
            " ``{current}`` placeholder, which will be replaced by ``output_current_text``. If ``{context}`` is"
            " also specified, target-side context will be used. Useful for models requiring special tokens or"
            " formatting in the output text (e.g. <brk> tags to separate context and current outputs)."
        ),
    )
    attribution_aggregators: List[str] = cli_arg(
        default_factory=list,
        help=(
            "The aggregators used to aggregate the attribution scores for each context. The outcome should"
            " produce one score per input token"
        ),
        choices=list_aggregators() + list_aggregation_functions(),
    )
    normalize_attributions: bool = cli_arg(
        default=False,
        help=(
            "Whether to normalize the attribution scores for each context. If ``True``, the attribution scores "
            "for each context are normalized to sum up to 1, providing a relative notion of input salience."
        ),
    )
    context_sensitivity_std_threshold: float = cli_arg(
        default=1.0,
        help=(
            "Parameter to control the number of standard deviations used as threshold to select "
            "context-sensitive tokens."
        ),
    )
    context_sensitivity_topk: Optional[int] = cli_arg(
        default=None,
        help=(
            "Parameter to select only the top K elements from the available context-sensitive generated tokens. If"
            " ``context_sensitivity_std_threshold`` is also specified, the top K tokens are selected from the"
            " context-sensitive tokens that pass the threshold."
        ),
    )
    attribution_std_threshold: float = cli_arg(
        default=1.0,
        help=(
            "Parameter to control the number of standard deviations used as threshold to select attributed tokens"
            " in the context."
        ),
    )
    attribution_topk: Optional[int] = cli_arg(
        default=None,
        help=(
            "Parameter to select only the top K elements from the available attributed context tokens. If"
            " ``attribution_std_threshold`` is also specified, the top K tokens are selected from the attributed"
            " context tokens that pass the threshold.",
        ),
    )
    align_output_context_auto: bool = cli_arg(
        default=False,
        help=(
            "Argument used for encoder-decoder model when generating text with an output template including both"
            " {context} and {current}, to attempt an automatic detection of which portions of the output belong to"
            " context vs. current in absence of other explicit cues. If set to True, the input and output context"
            " and current texts are aligned automatically (assuming an MT-like task), and the alignments are "
            " assumed to be valid to separate the two without further user validation. Otherwise, the user is "
            " prompted to manually specify which part of the generated text corresponds to the output context."
        ),
    )
    special_tokens_to_keep: List[str] = cli_arg(
        default_factory=list,
        help="Special tokens to preserve in the generated string, e.g. as separator between context and current.",
    )
    show_intermediate_outputs: bool = cli_arg(
        default=False,
        help="If specified, the intermediate outputs of the two steps are shown.",
    )

    def __post_init__(self):
        if not self.input_current_text:
            raise ValueError("--input_current_text must be a non-empty string.")
        if self.input_context_text and "{context}" not in self.input_template:
            logger.warning(
                f"input_template has format {self.input_template} (no {{context}}), but --input_context_text is"
                " specified. Ignoring provided --input_context_text."
            )
            self.input_context_text = None
        if self.output_context_text and "{context}" not in self.output_template:
            logger.warning(
                f"output_template has format {self.output_template} (no {{context}}), but --output_context_text is"
                " specified. Ignoring provided --output_context_text."
            )
            self.output_context_text = None
        if not self.input_context_text and "{context}" in self.input_template:
            raise ValueError(
                f"{{context}} format placeholder is present in input_template {self.input_template},"
                " but --input_context_text is not specified."
            )
        if "{current}" not in self.input_template:
            raise ValueError(f"{{current}} format placeholder is missing from input_template {self.input_template}.")
        if not self.input_current_text:
            raise ValueError("--input_current_text must be a non-empty string.")
        if "{current}" not in self.output_template:
            raise ValueError(f"{{current}} format placeholder is missing from output_template {self.output_template}.")


def format_template(template: str, current: str, context: Optional[str] = None) -> str:
    kwargs = {"current": current}
    if context is not None:
        kwargs["context"] = context
    return template.format(**kwargs)


def generate_model_output(
    model: AttributionModel,
    model_input: str,
    generation_kwargs: Dict[str, Any],
    special_tokens_to_keep: List[str],
    output_template: str,
    prefix: str,
    suffix: str,
) -> str:
    # Generate outputs, strip special tokens and remove prefix/suffix
    output_gen = model.generate(model_input, skip_special_tokens=False, **generation_kwargs)[0]
    output_tokens = [
        t
        for t in model.convert_string_to_tokens(output_gen, skip_special_tokens=False)
        if t not in model.special_tokens or t in special_tokens_to_keep
    ]
    output_gen = model.convert_tokens_to_string(output_tokens, skip_special_tokens=False)

    if prefix:
        if not output_gen.startswith(prefix):
            raise ValueError(
                f"Output template {output_template} contains prefix {prefix} but output '{output_gen}' does"
                " not match the prefix. Please check whether the template is correct, or force context/current"
                " outputs."
            )
        output_gen = output_gen[len(prefix) :]
    if suffix:
        if not output_gen.endswith(suffix):
            raise ValueError(
                f"Output template {output_template} contains suffix {suffix} but output '{output_gen}' does"
                " not match the suffix. Please check whether the template is correct, or force context/current"
                " outputs."
            )
        output_gen = output_gen[: -len(suffix)]
    return output_gen


def prompt_user_for_context(output: str, context_candidate: Optional[str] = None) -> str:
    """Prompt the user to provide the correct context for the provided output."""
    while True:
        if context_candidate:
            is_correct_candidate = Confirm.ask(
                f'\n:arrow_right: The model generated the following output: "[bold]{output}[/bold]"'
                f'\n:question: Is [bold]"{context_candidate}"[/bold] the correct context you want to attribute?'
            )
        if is_correct_candidate:
            user_context = context_candidate
        else:
            user_context = Prompt.ask(
                ":writing_hand: Please enter the portion of the generated output representing the correct context"
            )
        if user_context in output and user_context.strip():
            break
        rprint(
            "[prompt.invalid]The provided context is invalid. Please provide a non-empty substring of"
            " the model output above to use as context."
        )
    return user_context


def get_output_context_from_aligned_inputs(input_context: str, output_text: str) -> str:
    """Retrieve the output context from alignments between input context and the full output text."""
    aligned_context = compute_word_aligns(input_context, output_text, split_pattern=r"\s+|\b")
    max_context_id = max(pair[1] for pair in aligned_context.alignments)
    output_text_boundary_token = aligned_context.target_tokens[max_context_id]
    # Empty spans correspond to token boundaries
    spans = [m.span() for m in re.finditer(r"\s+|\b", output_text)]
    tok_start_positions = list({start if start == end else end for start, end in spans})
    output_text_context_candidate_boundary = tok_start_positions[max_context_id] + len(output_text_boundary_token)
    return output_text[:output_text_context_candidate_boundary]


def prepare_outputs(
    model: AttributionModel,
    input_context_text: Optional[str],
    input_full_text: str,
    output_context_text: Optional[str],
    output_current_text: Optional[str],
    output_template: str,
    align_output_context_auto: bool = False,
    generation_kwargs: Dict[str, Any] = {},
    special_tokens_to_keep: List[str] = [],
) -> Tuple[Optional[str], str]:
    """Handle model outputs and prepare them for attribution.
    This procedure is valid both for encoder-decoder and decoder-only models.

    | use_context | has_ctx | has_curr | setting
    |-------------|---------|----------|--------
    | True        | True    | True     | 1. Use forced context + current as output
    | False       | False   | True     | 2. Use forced current as output
    | True        | True    | False    | 3. Set inputs with forced context, generate output, use as current
    | False       | False   | False    | 4. Generate output, use it as current
    | True        | False   | False    | 5. Generate output, handle context/current splitting
    | True        | False   | True     | 6. Generate output, handle context/current splitting, force current

    NOTE: If ``use_context`` is True but ``has_ctx`` is False, the model generation is assumed to contain both a
    context and a current portion which need to be separated. ``has_ctx`` cannot be True if ``use_context`` is False
    (pre-check in ``__post_init__``).
    """
    use_context = "{context}" in output_template
    has_ctx = output_context_text is not None
    has_curr = output_current_text is not None
    model_input = input_full_text
    final_current = output_current_text
    final_context = output_context_text

    # E.g. output template "A{context}B{current}C" -> prefix = "A", suffix = "C", separator = "B"
    prefix, *_ = output_template.partition("{context}" if use_context else "{current}")
    *_, suffix = output_template.partition("{current}")
    separator = output_template.split("{context}")[1].split("{current}")[0] if use_context else None

    # Settings 1, 2
    if (has_ctx == use_context) and has_curr:
        return final_context, final_current

    # Prepend output prefix and context, if available, if current output needs to be generated
    if has_ctx and not has_curr:
        if model.is_encoder_decoder:
            generation_kwargs["decoder_input_ids"] = model.encode(
                prefix + output_context_text, as_targets=True, add_special_tokens=False
            ).input_ids
        else:
            model_input = input_full_text + prefix + output_context_text

    output_gen = generate_model_output(
        model, model_input, generation_kwargs, special_tokens_to_keep, output_template, prefix, suffix
    )

    # Settings 3, 4
    if (has_ctx == use_context) and not has_curr:
        if use_context:
            if model.is_encoder_decoder:
                final_current = output_gen[len(output_context_text + separator) :]
            else:
                final_current = output_gen[len(model_input + separator) :]
        else:
            final_current = output_gen if model.is_encoder_decoder else output_gen[len(model_input) :]
        return final_context, final_current

    # Settings 5, 6
    # Try splitting the output into context and current text using ``separator``. As we have no guarantees of its
    # uniqueness (e.g. it could be whitespace, also found between tokens in context and current) we consider the
    # splitting successful if exactly 2 substrings are produced. If this fails, we try splitting on punctuation.
    output_context_candidate = None
    separator_split_context_current_substring = output_gen.split(separator)
    if len(separator_split_context_current_substring) == 2:
        output_context_candidate = separator_split_context_current_substring[0]
    if not output_context_candidate:
        punct_expr = re.compile(r"[\s{}]+".format(re.escape(".?!,;:)]}")))
        punctuation_split_context_current_substring = [s for s in punct_expr.split(output_gen) if s]
        if len(punctuation_split_context_current_substring) == 2:
            output_context_candidate = punctuation_split_context_current_substring[0]

    # Final resort: if the model is an encoder-decoder model, we align the full input and full output, identifying
    # which tokens correspond to context and which to current. This assumes that input and output texts are alignable
    # (e.g. translations of each other). We prompt the user a yes/no question asking whether the context identified is
    # correct. If not, the user is asked to provide the correct context. If align_output_context_auto = True, aligned
    # texts are assumed to be correct (no user input required, to automate the procedure)
    if not output_context_candidate and model.is_encoder_decoder and input_context_text is not None:
        output_context_candidate = get_output_context_from_aligned_inputs(input_context_text, output_gen)

    if output_context_candidate and align_output_context_auto:
        final_context = output_context_candidate
    else:
        final_context = prompt_user_for_context(output_gen, output_context_candidate)
    template_output_context = output_template.split("{current}")[0].format(context=final_context)
    final_current = output_gen[min(len(template_output_context), len(output_gen)) :]
    if not has_curr and not final_current:
        raise ValueError(
            f"The model produced an empty current output given the specified context '{final_context}'. If no"
            " context is generated naturally by the model, you can force an output context using the"
            " --output_context_text option."
        )
    if has_curr:
        logger.warning(
            f"The model produced current text '{final_current}', but the specified output_current_text"
            f" '{output_current_text}'is used instead. If you want to use the original current output text generated"
            " by the model, remove the --output_current_text option."
        )
    return final_context, final_current


def attribute_context(args: AttributeContextArgs):
    model = HuggingfaceModel.load(
        args.model_name_or_path,
        args.attribution_method,
        model_kwargs=args.model_kwargs,
        tokenizer_kwargs=args.tokenizer_kwargs,
    )
    # Handle language tag for multilingual models - no need to specify it in generation kwargs
    if "tgt_lang" in args.tokenizer_kwargs and "forced_bos_token_id" not in args.generation_kwargs:
        tgt_lang = args.tokenizer_kwargs["tgt_lang"]
        args.generation_kwargs["forced_bos_token_id"] = model.tokenizer.lang_code_to_id[tgt_lang]

    # Prepare input/outputs (generate if necessary)
    in_text = format_template(args.input_template, args.input_current_text, args.input_context_text)
    args.output_context_text, args.output_current_text = prepare_outputs(
        model=model,
        input_context_text=args.input_context_text,
        input_full_text=in_text,
        output_context_text=args.output_context_text,
        output_current_text=args.output_current_text,
        output_template=args.output_template,
        align_output_context_auto=args.align_output_context_auto,
        generation_kwargs=args.generation_kwargs,
        special_tokens_to_keep=args.special_tokens_to_keep,
    )
    out_text = format_template(args.output_template, args.output_current_text, args.output_context_text)

    # Step 1: Context-sensitive Target Identification
    cti_out = model.attribute(
        args.input_current_text,
        args.output_current_text,
        attribute_target=model.is_encoder_decoder,
        step_scores=[args.context_sensitivity_metric],
        contrast_sources=in_text if model.is_encoder_decoder else None,
        contrast_targets=out_text,
        show_progress=False,
        method="dummy",
    )[0]
    if args.show_intermediate_outputs:
        cti_out.show(do_aggregation=False)

    # If using tgt language tag, ignore its context-sensitive score since its contrastive comparison is not meaningful.
    # TODO

    # Tokenize inputs and outputs
    # tokenize_inputs_outputs()
    # Apply context sensitivity metric
    # detect_context_sensitive_tokens()
    # Discretize context sensitivity scores to obtain tags
    # scores_to_tags()
    #
    # For every tagged output token:
    #   If the attribution method supports contrastive inputs, prepare contextless generation, align it with current
    #   one
    #   and identify the start position for tokens belonging to the current output
    #   prepare_contrastive_inputs()
    #   Attribute the tagged output token to input (and output) context
    #   attribute_context_sensitive_token()
    #   Aggregate output scores
    #   aggregate_attribution_scores()
    #   Discretize attribution scores to obtain tags
    #   scores_to_tags()
    # Return JSON output with detected tokens


class AttributeContextCommand(BaseCLICommand):
    _name = "attribute-context"
    _help = "Detect context-sensitive tokens in a generated text and attribute their predictions to input context."
    _dataclasses = AttributeContextArgs

    def run(args: AttributeContextArgs):
        attribute_context(args)
